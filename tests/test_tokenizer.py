from scratchgpt.tokenizer.char_tokenizer import CharTokenizer, Utf8Tokenizer


test_data = [
    "ĞŸÑ€Ğ¸Ğ²ĞµÑ‚, ĞºĞ°Ğº Ğ´ĞµĞ»Ğ°? ğŸ˜Š", 
    "Ğ¯ Ğ»ÑĞ±Ğ»Ñ Ñ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ ĞºĞ½Ğ¸Ğ³Ğ¸.", 
    "ĞœĞ¾ÑĞºĞ²Ğ° - ÑÑ‚Ğ¾ ĞºÑ€Ğ°ÑĞ¸Ğ²Ñ‹Ğ¹ Ğ³Ğ¾Ñ€Ğ¾Ğ´.", 
    "ì•ˆë…•í•˜ì„¸ìš” ğŸ‘‹", 
    "ë‚˜ëŠ” ë‹¹ì‹ ì„ ë§Œë‚˜ì„œ í–‰ë³µí•´ìš” ğŸ˜Š", 
    "ì„œìš¸ì€ ì•„ë¦„ë‹¤ìš´ ë„ì‹œì…ë‹ˆë‹¤.", 
    "Ciao, come stai? ğŸ˜Š", 
    "Amo leggere libri.", 
    "Roma Ã¨ una cittÃ  bellissima.", 
    "Hello, how are you? ğŸ‘‹", 
    "I love to read books.", 
    "New York City is a bustling metropolis ğŸ—½ï¸"
]

def test_basic_char_tokenizer():
    test_corpus = "".join(test_data)
    tokenizer = CharTokenizer(test_corpus)

    for test_sample in test_data:
        encoded = tokenizer.encode(test_sample)
        decoded = tokenizer.decode(encoded)

        assert test_sample == decoded, "Oh no, this thing failed"


def test_utf8_tokenizer():
    tokenizer = Utf8Tokenizer()

    for test_sample in test_data:
        encoded = tokenizer.encode(test_sample)
        decoded = tokenizer.decode(encoded)

        assert test_sample == decoded, "Oh no, Utf8Tokenizer failed"

    print(f"{tokenizer.vocab_size=}")
